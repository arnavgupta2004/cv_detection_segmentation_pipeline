# Real-Time Object Detection and Instance Segmentation Pipeline

A robust, real-time computer vision pipeline demonstrating both **Object Detection** (using YOLOv8) and **Instance Segmentation** (using a custom-built U-Net from scratch in PyTorch). 

The goal of this project is to provide a clean, interview-ready reference implementation for:
1. End-to-end training pipelines for both detection and segmentation.
2. Building and understanding a U-Net architecture completely from scratch.
3. Incorporating data augmentation (`albumentations`).
4. Real-time multi-model inference via OpenCV.

## Structure 

```text
computer_vision_pipeline/
├── src/
│   ├── preprocess.py       # Resizing, normalizing, tensor formatting
│   ├── augment.py          # Albumentations (Flip, Rotate, Blur, Cutout)
│   ├── train_yolo.py       # Fine-tunes YOLOv8n on COCO128
│   ├── train_unet.py       # Trains U-Net on a synthetic shape dataset
│   ├── unet_model.py       # PyTorch U-Net architecture (from scratch!)
│   ├── evaluate.py         # Computes IoU, Dice Score, and mAP metrics
│   └── inference.py        # Real-time webcam / video inference
├── runs/                   # Auto-generated by scripts to store weights & logs
├── requirements.txt
└── README.md
```

## Setup Instructions

1. **Clone/Navigate to the directory**:
   ```bash
   cd computer_vision_pipeline
   ```

2. **Install Dependencies**:
   Ensure you have Python 3.8+ installed. Then simply run:
   ```bash
   pip install -r requirements.txt
   ```
   *(Note: The COCO128 dataset used for YOLO training will be automatically downloaded by the `ultralytics` package upon first run, so no manual dataset setup is required!)*

## Usage Guide

### 1. Training YOLO (Object Detection)
Trains a YOLOv8 Nano model on the COCO128 dataset for 10 epochs.
```bash
python src/train_yolo.py
```
* **Output**: The best model weights will be saved to `runs/yolo/yolov8n_coco128/weights/best.pt`.

### 2. Training U-Net (Instance Segmentation)
Trains our from-scratch U-Net on a consistently generated synthetic shape dataset to guarantee immediate reproducibility without external dependencies.
```bash
python src/train_unet.py
```
* **Output**: Uses a combined BCE + Dice loss. The best weights are saved to `runs/unet/best_unet.pth`.

### 3. Evaluation
Evaluates both models and computes comprehensive metrics (mAP@50, mAP@50-95, IoU, Dice Score).
```bash
python src/evaluate.py
```

### 4. Real-Time Inference
Runs the dual-model inference pipeline using OpenCV. It applies YOLO object detection and U-Net instance segmentation sequentially per frame.

**Using Webcam (Default)**:
```bash
python src/inference.py --source 0 --mode both
```
**Using a Video File**:
```bash
python src/inference.py --source path/to/video.mp4 
```
**Using an Image**:
```bash
python src/inference.py --source path/to/image.jpg
```
**Options**:
* `--mode detect` (only run YOLO)
* `--mode segment` (only run U-Net)
* `--mode both` (run both overlaid)

Press `q` to quit the inference window.

## Sample Output Description

During inference, you will see a unified real-time video stream showing:
- **Red text**: Frame processing speed (FPS) in the top-left corner.
- **Blue Bounding Boxes**: Around the detected COCO shapes.
- **Blue text**: Class names and detection confidence (e.g. `person 0.85`).
- **Green Semi-Transparent Overlay**: The segmented structural mask denoting the pixel-perfect bounds of shapes detected by U-Net.

## Metrics Details

Because the project focuses on educational boilerplate execution and minimal dataset training to guarantee instant setup:
- We expect **YOLOv8** mAP@50 to be above 0.50 even on just 10 epochs of COCO128.
- We expect **U-Net** semantic capabilities (Mean IoU and Dice) to quickly climb >0.80 due to the stable synthetic shapes.
